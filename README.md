# Amith Vardhan Reddy Surasani

I am an emerging data scientist skilled in leveraging advanced techniques in data analytics, machine learning, and deep learning to extract meaningful patterns, relationships, and insights from complex datasets. My programming expertise includes Python, Java, and SQL, enabling efficient data processing, model development, and deployment.

Additionally, I am proficient in visualization tools such as Excel and Tableau, allowing me to create interactive dashboards and reports to communicate insights effectively to stakeholders. My specialization extends to advanced areas such as Natural Language Processing (NLP), where I analyze textual data to uncover actionable insights. I also have experience in web scraping, utilizing tools such as BeautifulSoup, Selenium, and Google Chrome WebDriver, as well as APIs from various platforms.

Moreover, I have a strong foundation in Generative AI, including the use of Large Language Models (LLMs) like GPT and Llama 3 for generating high-quality textual data. My expertise with the Groq API enables me to enhance the performance and scalability of data analytics and machine learning workflows, optimizing efficiency and productivity in AI-driven tasks.

Currently, I am a data scientist at DUTA where I fine-tune GPT 3.5 and Llama 3 with cleaned articles (which are in tens of thousands) in English, Telugu and Hindi gathered from various news sources; and building a chatbot which uses these fine-tune models for retrieving response related to news according to user's prompt.

I hold a master's degree in Information Systems Technologies at Wilmington University, where I acquired knowledge in data analysis using SQL and wrote a research project on how the cloud ransomware can be prevented using deep learning techniques. I also graduated with a bachelors' degree in Electronics and Communication Engineering from MVSR Engineering College affiliated to Osmania University. In my undergraduate curriculum, I attained understanding on how an image can be analyzed from a course named Digital Image Processing, which involves concepts such as Morphological Processing, Image Segmentation, Median Filtering, Edge Detection (Canny, Prewitt etc.), Image Transformation, One-Hot Encoding, and many more. My undergraduate capstone project involved leveraging Convolutional Neural Networks (CNNs) to detect brain tumors from MRI images. This project was implemented in MATLAB, and it strengthened my practical knowledge of machine learning techniques applied to image analysis. Apart from these also gained expertise in Java and Python, which were integral components of my undergraduate curriculum.

To enhance my skills and practical experience in the field of Data Science, I actively work on real-time projects from various websites such as Kaggle, DataCamp, HiCounselor, Udemy and many more. My work on these projects can be explored on my GitHub profile.

## Academic Qualifications

#### Wilmington University
Master of Science in Information Systems Technologies\
August 2022 - December 2023\
Grade: 3.61

#### MVSR Engineering College, Osmania University
Bachelor of Engineering in Electronics and Communication Engineering\
August 2017 - December 2021\
Grade: 3.0

## Work Experience

#### Data Scientist
DUTA - Part-Time\
July 2024 - Present

DUTA is the upcoming Instagram-like news aggregation app targeted towards people in two Telugu states - Telangana and Andhra Pradesh.  It displays news updates and their summaries on the feed, and allows the user to react to these articles - like, share, subscribe, comment etc in three languages - English, Telugu, Hindi. DUTA aims at display of news articles from any source in the language of user's choice (English, Telugu, Hindi), display of only essential insights of an article on the feed, to prevent information overload in users, enabling users to react and comment to the articles, thereby dealing with any political or ideological bias by news sources, availability of a chatbot that generates answers related to news articles according to the prompt given by the user in any language.

We extracted and cleaned around ten thousand articles, their titles, and their summaries from various news sources available in all three languages using Python libraries such as BeautifulSoup, Selenium, Google Chrome WebDriver, and individual news source APIs (such as Inshorts API). Their respective summaries were not found for certain summaries; hence, we generated summaries using the Groq API. We then translated these articles into two languages other than their respective languages in which they were originally written. For example, if an article was originally in English, it was translated into Telugu and Hindi. If the original language is Telugu, the article is translated into English, Hindi, and so on. We then went ahead and fine-tuned the GPT 3.5 and Llama 3 models with these articles, their titles, and their summaries. This is no other than making our own version of the GPT 3.5, specifically for dealing with articles. This is the version of GPT 3.5, which we incorporated into our chatbot. We also went ahead and fine-tuned GPT 3.5 with articles and their summaries in each language, to design models that can summarize articles in their respective languages. For example, we designed the English summarization GPT 3.5, by fine-tuning it with English articles and their summaries only, and we designed Telugu summarization GPT 3.5, by fine-tuning it with Telugu articles and their summaries only.


## Projects
